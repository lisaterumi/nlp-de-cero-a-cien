# Generación de textos con con Transformers

Esta semana el tema fue generación de textos, un concepto fundamental de Transformers famosos como el GPT-2 y GPT-3. Aquí les compartimos recursos adicionales así como publicaciones por si les interesa entrar con mayor profundidad en el tema. Así mismo, hay un cuaderno de colab para que puedan entender mejor el concepto.

## Recursos adicionales

* [The Illustrated GPT-2 (Visualizing Transformer Language Models)](http://jalammar.github.io/illustrated-gpt2/): Esta es una guía ilustrada que explica de manera muy intuitiva los conceptos de decoders y generacion de textos.
* [Natural Language Processing With Transformers](https://learning.oreilly.com/library/view/natural-language-processing/9781098103231/ch06.html): Un capítulo del próximo libro sobre Transformers y en el que se basó esta sesión.
* [How to generate text: using different decoding methods for language generation with Transformers](https://huggingface.co/blog/how-to-generate): Un recorrido detallado sobre varias formas de generar texto

## Papers

* [Better Language Models and Their Implications](https://openai.com/blog/better-language-models/). Tal vez sea la primera vez que un transformador aparece en las noticias.
* [Language Models are Few-Shot Learners](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf): Tal vez el modelo de lenguaje más potente del mundo en este momento, GPT-3 muestra interesantes capacidades como el aprendizaje de cero y pocos demostraciones.
* [Evaluating Large Language Models Trained on Code](https://arxiv.org/abs/2107.03374): El modelo detrás del GitHub Copilot
